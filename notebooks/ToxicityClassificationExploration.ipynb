{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/konstfed/Documents/Study/PMLDL/PMLDL-TextDetoxification\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "os.chdir(os.path.dirname(os.path.abspath(\"./\")))\n",
    "print(os.getcwd())\n",
    "\n",
    "# SCRIPT_DIR = os.path.abspath(\".\")\n",
    "# print(SCRIPT_DIR)\n",
    "# sys.path.append(SCRIPT_DIR)\n",
    "# sys.path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/konstfed/Documents/Study/PMLDL/PMLDL-TextDetoxification/toxic-venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import yaml\n",
    "from addict import Dict\n",
    "\n",
    "from src.models import build_model\n",
    "from src.inference import ToxicClassificationPipeline, BertPipeline\n",
    "from src import preprocessing\n",
    "\n",
    "sys.modules['preprocessing'] = preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "with open(\"configs/sentence_toxic_cls/hard_labels.yaml\", \"r\") as f:\n",
    "    hard_config = Dict(yaml.safe_load(f))\n",
    "\n",
    "with open(\"configs/sentence_toxic_cls/soft_labels.yaml\", \"r\") as f:\n",
    "    soft_config = Dict(yaml.safe_load(f))\n",
    "\n",
    "with open(\"configs/sentence_toxic_cls/distilbert_inference.yaml\", \"r\") as f:\n",
    "    bert_config = Dict(yaml.safe_load(f))\n",
    "\n",
    "\n",
    "soft_cls = ToxicClassificationPipeline(soft_config) # BCEloss 0.45 on test\n",
    "hard_cls = ToxicClassificationPipeline(hard_config) # BCEloss 0.46 on test\n",
    "bert_cls = BertPipeline(bert_config) # BCEloss 0.05 on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts2analyze = [\n",
    "    \"Dear professor, For starters, We would like to emphasize that we greatly appreciate your collaboration and understanding so far: Extending the deadline of the first assignment, agreeing to the online format, answering our questions in the group chat (just to name a few). Furthermore, we cannot stress enough that this is by no means personal. The general attendance rate is low, due to multiple incremental factors like students having to work to support themselves, regardless of the course in question and if attendance is mandatory or not. We would also like to note that last week was midterms week with overlapping deadlines and that we don’t have a rich mathematical background like the students of MIPT, due to the industrial/practical nature of most courses at Innopolis; so, sometimes it is wiser to skip lecture and dedicate more time to go over not only lecture materials but necessary prerequisites to fill our knowledge gaps. It’s also important to note that if this course wasn’t important to us, there wouldn’t be that many students signing up for it (this is double the usual amount). We’re just doing the best we can with the starting state we were in and with the other workload of university/life.\",\n",
    "    \"Fucking kill you, For starters, We would like to emphasize that we greatly appreciate your collaboration and understanding so far: Extending the deadline of the first assignment, agreeing to the online format, answering our questions in the group chat (just to name a few). Furthermore, we cannot stress enough that this is by no means personal. The general attendance rate is low, due to multiple incremental factors like students having to work to support themselves, regardless of the course in question and if attendance is mandatory or not. We would also like to note that last week was midterms week with overlapping deadlines and that we don’t have a rich mathematical background like the students of MIPT, due to the industrial/practical nature of most courses at Innopolis; so, sometimes it is wiser to skip lecture and dedicate more time to go over not only lecture materials but necessary prerequisites to fill our knowledge gaps. It’s also important to note that if this course wasn’t important to us, there wouldn’t be that many students signing up for it (this is double the usual amount). We’re just doing the best we can with the starting state we were in and with the other\",\n",
    "    \"If you have not time to grade our works, why you think that we have enough time to write 7 pages of report with full-time work?\",\n",
    "    \"Also no bonus points for late grading. Half of works are not graded. The worst course at this moment\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1. soft label: tensor([6.4218e-05]), bert label: tensor([[0.5429]])\n",
      "Text 2. soft label: tensor([0.0723]), bert label: tensor([[0.7223]])\n",
      "Text 3. soft label: tensor([0.0757]), bert label: tensor([[0.5027]])\n",
      "Text 4. soft label: tensor([0.4062]), bert label: tensor([[0.5116]])\n"
     ]
    }
   ],
   "source": [
    "for i, text in enumerate(texts2analyze,start=1):\n",
    "    soft_res = soft_cls.forward(text)\n",
    "    bert_res = bert_cls.forward(text)\n",
    "    print(f\"Text {i}. soft label: {soft_res}, bert label: {bert_res}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toxic-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
